{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DBRF import DBRF\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from datetime import date\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in listdir('classification_datasets') if isfile(join('classification_datasets', f))]\n",
    "all_files = sorted(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df = pd.get_dummies(df, columns=[col])\n",
    "            \n",
    "    if df[df.columns[-1]].dtype == object:\n",
    "        last_col = df.columns[-1]\n",
    "        df['new_class'] = pd.factorize(df[last_col])[0]\n",
    "        df = df[[i for i in df.columns if i != last_col]]\n",
    "            \n",
    "    df = df.interpolate()\n",
    "    df.fillna(value=0.0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_params(D, file_name):\n",
    "    \n",
    "    Data_params = []\n",
    "    \n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "\n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print('CV_num -',CV_num)\n",
    "        try:\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            try:\n",
    "                #Does not work for every Sklearn version\n",
    "                best_n, best_ests, best_crit = find_params(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                best_n, best_ests, best_crit = round(np.random.uniform(1,10)), round(np.random.uniform(5,100)), 'gini'\n",
    "\n",
    "            classifier = DBRF(best_n, best_ests, best_crit)\n",
    "\n",
    "            #time.process_time() will measure the CPU time\n",
    "            start1 = time.process_time()\n",
    "            classifier.fit(X_train, y_train)\n",
    "            train_time = time.process_time() - start1\n",
    "\n",
    "            #prediction\n",
    "            start2 = time.process_time()\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            predict_time = time.process_time() - start2\n",
    "\n",
    "    #         y_prob = classifier.predict_proba(X_test)\n",
    "\n",
    "            #dataframe size\n",
    "            r, c = X_test.shape\n",
    "\n",
    "            # Scale\n",
    "            infer_time = predict_time*(1000/r)\n",
    "\n",
    "            #Accuracy\n",
    "            #preds_classes = [x.index(max(x)) for x in prediction_probab]\n",
    "            acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            # TPR, FPR ,Precision\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "            # Sensitivity, hit rate, recall, or true positive rate\n",
    "            TPR = tp/(tp+fn)\n",
    "\n",
    "            # Fall out or false positive rate\n",
    "            FPR = fp/(fp+tn)\n",
    "\n",
    "\n",
    "            # Precision or positive predictive value\n",
    "            #Precision = tp/(tp+fp)\n",
    "            precision = precision_score(y_test, y_pred, average='binary')\n",
    "\n",
    "            score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            #Area under Precision-Recall Curve \n",
    "            average_precision = average_precision_score(y_test, y_pred)\n",
    "\n",
    "            hyper = 'n = '+str(round(best_n))+'. ests = '+str(round(best_ests))+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "            params = {'Dataset Name': file_name.rsplit('.',1)[0], \n",
    "                      'Algorithm Name': 'DBRF',\n",
    "                      'Cross Validation': CV_num, \n",
    "                      'Hyper-Parameters Values': hyper,\n",
    "                      'Accuracy':acc_score,\n",
    "                      'TPR':TPR,\n",
    "                      'FPR':FPR,\n",
    "                      'Precision':precision,\n",
    "                      'AUC':score,\n",
    "                      'PR-Curve':average_precision,\n",
    "                      'Training Time':train_time,\n",
    "                      'Inference Time':infer_time}\n",
    "\n",
    "            Data_params.append(params)\n",
    "            CV_num += 1\n",
    "        except:\n",
    "            continue\n",
    "    return Data_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_params(X_train, y_train):\n",
    "    clf = DBRF(n=5, n_estimators=50, criterion='gini')\n",
    "    param_dist = {'criterion': ['gini', 'entropy'],\n",
    "                  'n_estimators': stats.uniform(5, 30),\n",
    "                  'n' : stats.uniform(1, 10)}\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=50,cv=3, random_state=0, n_jobs=2)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    x = random_search.best_params_ \n",
    "    return x['n'], x['n_estimators'], x['criterion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_params(D, file_name, num_uni):\n",
    "    \n",
    "    Data_params = []\n",
    "\n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "    \n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        try:\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            try:\n",
    "                #Does not work for every Sklearn version\n",
    "                best_n, best_ests, best_crit = find_params(X_train, y_train)\n",
    "            except:\n",
    "                best_n, best_ests, best_crit = round(np.random.uniform(1,10)), round(np.random.uniform(5,100)), 'gini'\n",
    "\n",
    "            dbrf = DBRF(best_n, best_ests, best_crit)\n",
    "\n",
    "            classifier  = OneVsRestClassifier(dbrf)\n",
    "\n",
    "            #time.process_time() will measure the CPU time\n",
    "            start1 = time.process_time()\n",
    "            clf = classifier.fit(X_train, y_train)\n",
    "            train_time = time.process_time() - start1\n",
    "\n",
    "            #prediction\n",
    "            start2 = time.process_time()\n",
    "            y_pred = clf.predict(X_test)\n",
    "            predict_time = time.process_time() - start2\n",
    "\n",
    "\n",
    "            #dataframe size\n",
    "            r, c = X_test.shape\n",
    "\n",
    "            # Scale\n",
    "            infer_time = predict_time*(1000/r)\n",
    "\n",
    "            #Accuracy\n",
    "            #In binary and multiclass classification, this function is equal to the jaccard_score function (doc.)\n",
    "            acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            #auc calculate\n",
    "            y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "            new_test = np.zeros((y_test.size, num_uni+1))\n",
    "            new_test[np.arange(y_test.size),y_test] = 1\n",
    "\n",
    "            macro_roc_auc_ovr = roc_auc_score(new_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "            #Area under Precision-Recall Curve\n",
    "\n",
    "            #AP and the trapezoidal area under the operating points (sklearn.metrics.auc)\n",
    "            #are common ways to summarize a precision-recall curve that lead to different results.\n",
    "\n",
    "            # for use average_precision_score it is necessary to binarize the output of predict_proba\n",
    "            # predict_proba returns the probability of the sample for each class in the model,\n",
    "            #where classes are ordered as they are in self.classes\n",
    "\n",
    "\n",
    "            # binarize\n",
    "            label_y_test = label_binarize(y_test, classes= clf.classes_)\n",
    "\n",
    "            average_precision =  average_precision_score(label_y_test, y_prob, average='macro')\n",
    "\n",
    "\n",
    "            # TPR, FPR\n",
    "            # confusion matrix\n",
    "            cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            # computetion of TPR and FPR for each class\n",
    "            FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "            FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "            TP = np.diag(cnf_matrix)\n",
    "            TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "            FP = FP.astype(float)\n",
    "            FN = FN.astype(float)\n",
    "            TP = TP.astype(float)\n",
    "            TN = TN.astype(float)\n",
    "\n",
    "            # Sensitivity, hit rate, recall, or true positive rate\n",
    "            TPR = TP/(TP+FN)\n",
    "            # Fall out or false positive rate\n",
    "            FPR = FP/(FP+TN)\n",
    "            # Precision or positive predictive value\n",
    "            PPV = TP/(TP+FP)\n",
    "\n",
    "            # macro average\n",
    "            classes_num = clf.n_classes_\n",
    "            macro_TPR= sum(TPR)/classes_num\n",
    "            macro_FPR= sum(FPR)/classes_num\n",
    "            macro_PPV= sum(PPV)/classes_num\n",
    "\n",
    "            # Precision\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "            #save all calculations in a dictionary\n",
    "            hyper = 'n = '+str(best_n)+'. ests = '+str(best_ests)+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "\n",
    "            params = {'Dataset Name': file_name.rsplit('.',1)[0], \n",
    "                      'Algorithm Name': 'DBRF',\n",
    "                      'Cross Validation': CV_num, \n",
    "                      'Hyper-Parameters Values': hyper,\n",
    "                      'Accuracy':acc_score,\n",
    "                      'TPR':macro_TPR,\n",
    "                      'FPR':macro_FPR,\n",
    "                      'Precision':precision,\n",
    "                      'AUC':macro_roc_auc_ovr,\n",
    "                      'PR-Curve':average_precision,\n",
    "                      'Training Time':train_time,\n",
    "                      'Inference Time':infer_time}\n",
    "\n",
    "            Data_params.append(params)\n",
    "            CV_num += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return Data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9406a235d5064492a41fb3de52e361c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acute-nephritis.csv --- 120\n",
      "analcatdata_asbestos.csv --- 83\n",
      "analcatdata_boxing1.csv --- 120\n",
      "analcatdata_broadwaymult.csv --- 285\n",
      "analcatdata_germangss.csv --- 400\n",
      "analcatdata_lawsuit.csv --- 264\n",
      "ar4.csv --- 107\n",
      "autos.csv --- 205\n",
      "bank.csv --- 4521\n",
      "baseball.csv --- 1340\n",
      "blood.csv --- 748\n",
      "bodyfat.csv --- 252\n",
      "braziltourism.csv --- 412\n",
      "breast-cancer-wisc-diag.csv --- 569\n",
      "breast-cancer-wisc-prog.csv --- 198\n",
      "breast-cancer-wisc.csv --- 699\n",
      "breast-cancer.csv --- 286\n",
      "chatfield_4.csv --- 235\n",
      "chess-krvkp.csv --- 3196\n",
      "chscase_vine1.csv --- 52\n",
      "cloud.csv --- 108\n",
      "congressional-voting.csv --- 435\n",
      "conn-bench-sonar-mines-rocks.csv --- 208\n",
      "credit-approval.csv --- 690\n",
      "cylinder-bands.csv --- 512\n",
      "diabetes.csv --- 768\n",
      "diggle_table_a2.csv --- 310\n",
      "disclosure_z.csv --- 662\n",
      "echocardiogram.csv --- 131\n",
      "elusage.csv --- 55\n",
      "fertility.csv --- 100\n",
      "fri_c0_250_5.csv --- 250\n",
      "haberman-survival.csv --- 306\n",
      "heart-hungarian.csv --- 294\n",
      "hepatitis.csv --- 155\n",
      "hill-valley.csv --- 1212\n",
      "horse-colic.csv --- 368\n",
      "ilpd-indian-liver.csv --- 583\n",
      "ionosphere.csv --- 351\n",
      "kc3.csv --- 458\n",
      "kidney.csv --- 76\n",
      "labor.csv --- 57\n",
      "lowbwt.csv --- 189\n",
      "lupus.csv --- 87\n",
      "mammographic.csv --- 961\n",
      "meta.csv --- 528\n",
      "mfeat-morphological.csv --- 2000\n",
      "molec-biol-promoter.csv --- 106\n",
      "monks-1.csv --- 556\n",
      "monks-2.csv --- 601\n",
      "monks-3.csv --- 554\n",
      "mushroom.csv --- 8124\n",
      "musk-1.csv --- 476\n",
      "newton_hema.csv --- 140\n",
      "no2.csv --- 500\n",
      "oocytes_merluccius_nucleus_4d.csv --- 1022\n",
      "oocytes_trisopterus_nucleus_2f.csv --- 912\n",
      "ozone.csv --- 2536\n",
      "parkinsons.csv --- 195\n",
      "pima.csv --- 768\n",
      "pittsburg-bridges-T-OR-D_R.csv --- 102\n",
      "planning.csv --- 182\n",
      "plasma_retinol.csv --- 315\n",
      "pm10.csv --- 500\n",
      "prnn_synth.csv --- 250\n",
      "rabe_131.csv --- 50\n",
      "rmftsa_sleepdata.csv --- 1024\n",
      "schizo.csv --- 340\n",
      "schlvote.csv --- 38\n",
      "sleuth_case2002.csv --- 147\n",
      "socmob.csv --- 1156\n",
      "solar-flare.csv --- 1066\n",
      "spambase.csv --- 4601\n",
      "spect.csv --- 265\n",
      "spectf.csv --- 267\n",
      "squash-stored.csv --- 52\n",
      "squash-unstored.csv --- 52\n",
      "statlog-australian-credit.csv --- 690\n",
      "statlog-german-credit.csv --- 1000\n",
      "statlog-heart_.csv --- 270\n",
      "tae.csv --- 151\n",
      "tic-tac-toe.csv --- 958\n",
      "titanic.csv --- 2201\n",
      "transplant.csv --- 131\n",
      "triazines.csv --- 186\n",
      "vertebral-column-2clases.csv --- 310\n",
      "veteran.csv --- 137\n",
      "visualizing_livestock.csv --- 130\n",
      "vote.csv --- 435\n",
      "white-clover.csv --- 63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = dict()\n",
    "for file in tqdm(all_files):\n",
    "    if file not in all_results:\n",
    "    \n",
    "        D = load_data('classification_datasets/'+file)     \n",
    "\n",
    "\n",
    "        if D.nunique()[D.columns[-1]] > 2:\n",
    "            params = multi_params(D, file, D.nunique()[D.columns[-1]])\n",
    "\n",
    "        if D.nunique()[D.columns[-1]] == 2:\n",
    "            params = binary_params(D, file)\n",
    "\n",
    "        all_results[file] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b0e747fc904943864cc4b9a4b69230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('RESULTS_FILE.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    cols = ['Dataset Name', 'Algorithm Name','Cross Validation','Hyper-Parameters Values','Accuracy','TPR',\n",
    "            'FPR','Precision','AUC','PR-Curve','Training Time','Inference Time']\n",
    "    writer.writerow(cols)\n",
    "    for file in tqdm(all_results):\n",
    "        for line in all_results[file]:\n",
    "            writer.writerow(list(line.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
