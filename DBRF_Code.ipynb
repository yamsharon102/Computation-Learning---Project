{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DBRF import DBRF\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from datetime import date\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in listdir('classification_datasets') if isfile(join('classification_datasets', f))]\n",
    "all_files = sorted(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df = pd.get_dummies(df, columns=[col])\n",
    "            \n",
    "    if df[df.columns[-1]].dtype == object:\n",
    "        last_col = df.columns[-1]\n",
    "        df['new_class'] = pd.factorize(df[last_col])[0]\n",
    "        df = df[[i for i in df.columns if i != last_col]]\n",
    "            \n",
    "    df = df.interpolate()\n",
    "    df.fillna(value=0.0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_params(D, file_name):\n",
    "    \n",
    "    Data_params = []\n",
    "    \n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "\n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print('CV_num -',CV_num)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        try:\n",
    "            best_n, best_ests, best_crit = find_params(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            best_n, best_ests, best_crit = round(np.random.uniform(1,10)), round(np.random.uniform(5,100)), 'gini'\n",
    "            \n",
    "        classifier = DBRF(best_n, best_ests, best_crit)\n",
    "\n",
    "        #time.process_time() will measure the CPU time\n",
    "        start1 = time.process_time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        train_time = time.process_time() - start1\n",
    "\n",
    "        #prediction\n",
    "        start2 = time.process_time()\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        predict_time = time.process_time() - start2\n",
    "\n",
    "        #dataframe size\n",
    "        r, c = X_test.shape\n",
    "\n",
    "        # Scale\n",
    "        infer_time = predict_time*(1000/r)\n",
    "\n",
    "        #Accuracy\n",
    "        #preds_classes = [x.index(max(x)) for x in prediction_probab]\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # TPR, FPR ,Precision\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = tp/(tp+fn)\n",
    "\n",
    "        # Fall out or false positive rate\n",
    "        FPR = fp/(fp+tn)\n",
    "\n",
    "\n",
    "        # Precision or positive predictive value\n",
    "        #Precision = tp/(tp+fp)\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "\n",
    "        score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        #Area under Precision-Recall Curve \n",
    "        average_precision = average_precision_score(y_test, y_prob)\n",
    "\n",
    "        hyper = 'n = '+str(round(best_n))+'. ests = '+str(round(best_ests))+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "        params = {'Dataset Name': file_name, \n",
    "                  'Algorithm Name': 'DBRF',\n",
    "                  'Cross Validation': CV_num, \n",
    "                  'Hyper-Parameters Values': hyper,\n",
    "                  'Accuracy':acc_score,\n",
    "                  'TPR':TPR,\n",
    "                  'FPR':FPR,\n",
    "                  'Precision':precision,\n",
    "                  'AUC':score,\n",
    "                  'PR-Curve':average_precision,\n",
    "                  'Training Time':train_time,\n",
    "                  'Inference Time':infer_time}\n",
    "\n",
    "        Data_params.append(params)\n",
    "        CV_num += 1\n",
    "\n",
    "    return Data_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_params(X_train, y_train):\n",
    "    clf = DBRF(n=5, n_estimators=50, criterion='gini')\n",
    "    param_dist = {'criterion': ['gini', 'entropy'],\n",
    "                  'n_estimators': stats.uniform(5, 30),\n",
    "                  'n' : stats.uniform(1, 10)}\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=50,cv=3, random_state=0, n_jobs=2)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    x = random_search.best_params_ \n",
    "    return x['n'], x['n_estimators'], x['criterion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_params(D, file_name, num_uni):\n",
    "    \n",
    "    Data_params = []\n",
    "\n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "    \n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        try:\n",
    "            best_n, best_ests, best_crit = find_params(X_train, y_train)\n",
    "        except:\n",
    "            best_n, best_ests, best_crit = round(np.random.uniform(1,10)), round(np.random.uniform(5,100)), 'gini'\n",
    "\n",
    "        dbrf = DBRF(best_n, best_ests, best_crit)\n",
    "\n",
    "        classifier  = OneVsRestClassifier(dbrf)\n",
    "\n",
    "        #time.process_time() will measure the CPU time\n",
    "        start1 = time.process_time()\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "        train_time = time.process_time() - start1\n",
    "\n",
    "        #prediction\n",
    "        start2 = time.process_time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        predict_time = time.process_time() - start2\n",
    "\n",
    "\n",
    "        #dataframe size\n",
    "        r, c = X_test.shape\n",
    "\n",
    "        # Scale\n",
    "        infer_time = predict_time*(1000/r)\n",
    "\n",
    "        #Accuracy\n",
    "        #In binary and multiclass classification, this function is equal to the jaccard_score function (doc.)\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #auc calculate\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "        \n",
    "        new_test = np.zeros((y_test.size, num_uni+1))\n",
    "        new_test[np.arange(y_test.size),y_test] = 1\n",
    "\n",
    "        macro_roc_auc_ovr = roc_auc_score(new_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "        #Area under Precision-Recall Curve\n",
    "\n",
    "        #AP and the trapezoidal area under the operating points (sklearn.metrics.auc)\n",
    "        #are common ways to summarize a precision-recall curve that lead to different results.\n",
    "\n",
    "        # for use average_precision_score it is necessary to binarize the output of predict_proba\n",
    "        # predict_proba returns the probability of the sample for each class in the model,\n",
    "        #where classes are ordered as they are in self.classes\n",
    "\n",
    "\n",
    "        # binarize\n",
    "        label_y_test = label_binarize(y_test, classes= clf.classes_)\n",
    "\n",
    "        average_precision =  average_precision_score(label_y_test, y_prob, average='macro')\n",
    "\n",
    "\n",
    "        # TPR, FPR\n",
    "        # confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # computetion of TPR and FPR for each class\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = TP/(TP+FN)\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        # Precision or positive predictive value\n",
    "        PPV = TP/(TP+FP)\n",
    "\n",
    "        # macro average\n",
    "        classes_num = clf.n_classes_\n",
    "        macro_TPR= sum(TPR)/classes_num\n",
    "        macro_FPR= sum(FPR)/classes_num\n",
    "        macro_PPV= sum(PPV)/classes_num\n",
    "\n",
    "        # Precision\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        #save all calculations in a dictionary\n",
    "        hyper = 'n = '+str(best_n)+'. ests = '+str(best_ests)+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "\n",
    "        params = {'Dataset Name': file_name, \n",
    "                  'Algorithm Name': 'DBRF',\n",
    "                  'Cross Validation': CV_num, \n",
    "                  'Hyper-Parameters Values': hyper,\n",
    "                  'Accuracy':acc_score,\n",
    "                  'TPR':macro_TPR,\n",
    "                  'FPR':macro_FPR,\n",
    "                  'Precision':precision,\n",
    "                  'AUC':macro_roc_auc_ovr,\n",
    "                  'PR-Curve':average_precision,\n",
    "                  'Training Time':train_time,\n",
    "                  'Inference Time':infer_time}\n",
    "\n",
    "        Data_params.append(params)\n",
    "        CV_num += 1\n",
    "\n",
    "    return Data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = dict()\n",
    "for file in tqdm(all_files):\n",
    "    \n",
    "    D = load_data('classification_datasets/'+file)     \n",
    "\n",
    "    if D.nunique()[D.columns[-1]] > 2:\n",
    "        params = multi_params(D, file, D.nunique()[D.columns[-1]])\n",
    "\n",
    "    if D.nunique()[D.columns[-1]] == 2:\n",
    "        params = binary_params(D, file)\n",
    "\n",
    "    all_results[file] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RESULTS_FILE.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    cols = ['Dataset Name', 'Algorithm Name','Cross Validation','Hyper-Parameters Values','Accuracy','TPR',\n",
    "            'FPR','Precision','AUC','PR-Curve','Training Time','Inference Time']\n",
    "    writer.writerow(cols)\n",
    "    for file in tqdm(all_accs):\n",
    "        for line in all_accs[file]:\n",
    "            writer.writerow(list(line.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
