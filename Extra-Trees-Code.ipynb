{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "all_files = [f for f in listdir('classification_datasets') if isfile(join('classification_datasets', f))]\n",
    "all_files = sorted(all_files)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import csv\n",
    "from datetime import date\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df = pd.get_dummies(df, columns=[col])\n",
    "            \n",
    "    if df[df.columns[-1]].dtype == object:\n",
    "        last_col = df.columns[-1]\n",
    "        df['new_class'] = pd.factorize(df[last_col])[0]\n",
    "        df = df[[i for i in df.columns if i != last_col]]\n",
    "            \n",
    "    df = df.interpolate()\n",
    "    df.fillna(value=0.0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_params(D, file_name):\n",
    "    \n",
    "    Data_params = []\n",
    "    \n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "\n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print('CV_num -',CV_num)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        x = find_params(X_train, y_train)\n",
    "            \n",
    "        classifier = ExtraTreesClassifier(random_state=0, criterion=x['criterion'], n_estimators=x['n_estimators'])\n",
    "    \n",
    "        #time.process_time() will measure the CPU time\n",
    "        start1 = time.process_time()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        train_time = time.process_time() - start1\n",
    "\n",
    "        #prediction\n",
    "        start2 = time.process_time()\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        predict_time = time.process_time() - start2\n",
    "\n",
    "        #auc calculate\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "        #dataframe size\n",
    "        r, c = X_test.shape\n",
    "\n",
    "        # Scale\n",
    "        infer_time = predict_time*(1000/r)\n",
    "        \n",
    "        #Accuracy\n",
    "        #preds_classes = [x.index(max(x)) for x in prediction_probab]\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # TPR, FPR ,Precision\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = tp/(tp+fn)\n",
    "\n",
    "        # Fall out or false positive rate\n",
    "        FPR = fp/(fp+tn)\n",
    "\n",
    "        # Precision or positive predictive value\n",
    "        #Precision = tp/(tp+fp)\n",
    "        precision = precision_score(y_test, y_prob, average='binary')\n",
    "        \n",
    "        score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        #Area under Precision-Recall Curve \n",
    "        average_precision = average_precision_score(y_test, y_pred)\n",
    "\n",
    "        best_ests, best_crit = x['n_estimators'], x['criterion']\n",
    "        #save all calculations in a dictionary\n",
    "        hyper = 'ests = '+str(best_ests)+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "        params = {'Dataset Name': file_name, \n",
    "                  'Algorithm Name': 'EXTRA TREES',\n",
    "                  'Cross Validation': CV_num, \n",
    "                  'Hyper-Parameters Values': hyper,\n",
    "                  'Accuracy':acc_score,\n",
    "                  'TPR':TPR,\n",
    "                  'FPR':FPR,\n",
    "                  'Precision':precision,\n",
    "                  'AUC':score,\n",
    "                  'PR-Curve':average_precision,\n",
    "                  'Training Time':train_time,\n",
    "                  'Inference Time':infer_time}\n",
    "\n",
    "        Data_params.append(params)\n",
    "        CV_num += 1\n",
    "\n",
    "    return Data_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(X, y):\n",
    "    if not y.isnull().values.any():\n",
    "        return X, y\n",
    "    for idx, row in y.iterrows():\n",
    "        if y.iloc[idx] == np.nan:\n",
    "            y.drop(index=idx, inplace=True)\n",
    "            X.drop(index=idx, inplace=True)\n",
    "            \n",
    "    y = pd.factorize(y)\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_params(X_train, y_train):\n",
    "    clf = ExtraTreesClassifier(random_state=0)\n",
    "    param_dist = {'criterion': ['gini', 'entropy'],\n",
    "                  'n_estimators': stats.randint(5, 100)}\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=50,cv=3, random_state=0, n_jobs=2)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_one_class(new_test):\n",
    "    classes = np.zeros(len(new_test[0]))\n",
    "    for idx in range(len(classes)):\n",
    "        classes[idx] = sum([x[idx] for x in new_test])\n",
    "    num_change = 1\n",
    "    for idx in range(len(classes)):\n",
    "        if classes[idx] == 0:\n",
    "            to_add = np.zeros(len(new_test[0]))\n",
    "            to_add[idx] = 1.0\n",
    "            new_test[len(new_test)-num_change] = to_add\n",
    "            num_change += 1\n",
    "    return new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_params(D, file_name, num_uni):\n",
    "    \n",
    "    Data_params = []\n",
    "\n",
    "    X = D[D.columns[:-1]]\n",
    "    y = D[D.columns[-1]]\n",
    "    \n",
    "    \n",
    "    X, y = fix_data(X, y)\n",
    "    \n",
    "    #10-fold cross validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    CV_num = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print('CV_num -',CV_num)\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        x = find_params(X_train, y_train)\n",
    "            \n",
    "        clf = ExtraTreesClassifier(random_state=0, criterion=x['criterion'], n_estimators=x['n_estimators'])\n",
    "\n",
    "        classifier  = OneVsRestClassifier(clf)\n",
    "\n",
    "         #time.process_time() will measure the CPU time\n",
    "        start1 = time.process_time()\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "        train_time = time.process_time() - start1\n",
    "\n",
    "        #prediction\n",
    "        start2 = time.process_time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        predict_time = time.process_time() - start2\n",
    "\n",
    "\n",
    "        #dataframe size\n",
    "        r, c = X_test.shape\n",
    "\n",
    "        # Scale\n",
    "        infer_time = predict_time*(1000/r)\n",
    "\n",
    "        #Accuracy\n",
    "        #In binary and multiclass classification, this function is equal to the jaccard_score function (doc.)\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #auc calculate\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "        new_test = np.zeros((y_test.size, num_uni+1))\n",
    "        new_test[np.arange(y_test.size),y_test] = 1\n",
    "        \n",
    "        \n",
    "        macro_roc_auc_ovr = roc_auc_score(new_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "        #Area under Precision-Recall Curve\n",
    "\n",
    "        #AP and the trapezoidal area under the operating points (sklearn.metrics.auc)\n",
    "        #are common ways to summarize a precision-recall curve that lead to different results.\n",
    "\n",
    "        # for use average_precision_score it is necessary to binarize the output of predict_proba\n",
    "        # predict_proba returns the probability of the sample for each class in the model,\n",
    "        #where classes are ordered as they are in self.classes\n",
    "\n",
    "\n",
    "        # binarize\n",
    "        label_y_test = label_binarize(y_test, classes= clf.classes_)\n",
    "\n",
    "        average_precision =  average_precision_score(label_y_test, y_prob, average='macro')\n",
    "\n",
    "\n",
    "        # TPR, FPR\n",
    "        # confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # computetion of TPR and FPR for each class\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = TP/(TP+FN)\n",
    "        # Fall out or false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        # Precision or positive predictive value\n",
    "        PPV = TP/(TP+FP)\n",
    "\n",
    "        # macro average\n",
    "        classes_num = clf.n_classes_\n",
    "        macro_TPR= sum(TPR)/classes_num\n",
    "        macro_FPR= sum(FPR)/classes_num\n",
    "        macro_PPV= sum(PPV)/classes_num\n",
    "\n",
    "        # Precision\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        best_ests, best_crit = x['n_estimators'], x['criterion']\n",
    "        #save all calculations in a dictionary\n",
    "        hyper = 'ests = '+str(best_ests)+'. criterion = '+str(best_crit)+\".\"\n",
    "\n",
    "\n",
    "        params = {'Dataset Name': file_name, \n",
    "                  'Algorithm Name': 'EXTRA TREES',\n",
    "                  'Cross Validation': CV_num, \n",
    "                  'Hyper-Parameters Values': hyper,\n",
    "                  'Accuracy':acc_score,\n",
    "                  'TPR':macro_TPR,\n",
    "                  'FPR':macro_FPR,\n",
    "                  'Precision':precision,\n",
    "                  'AUC':macro_roc_auc_ovr,\n",
    "                  'PR-Curve':average_precision,\n",
    "                  'Training Time':train_time,\n",
    "                  'Inference Time':infer_time}\n",
    "#             print(params)\n",
    "\n",
    "        Data_params.append(params)\n",
    "        CV_num += 1\n",
    "\n",
    "    return Data_params\n",
    "    \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_prob(y_prob, y_test_uni, classes):\n",
    "    ret_arr = []\n",
    "    for prob in y_prob:\n",
    "        to_add = np.zeros(len(classes))\n",
    "        for idx, uni in enumerate(y_test_uni):\n",
    "            to_add[classes.index(uni)] = prob[idx]\n",
    "        ret_arr.append(to_add)\n",
    "    return np.array(ret_arr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = dict()\n",
    "for file in tqdm(all_files):\n",
    "    D = load_data('classification_datasets/'+file)     \n",
    "\n",
    "    if D.nunique()[D.columns[-1]] > 2:\n",
    "        params = multi_params(D, file, D.nunique()[D.columns[-1]])\n",
    "\n",
    "    if D.nunique()[D.columns[-1]] == 2:\n",
    "        params = binary_params(D, file)\n",
    "\n",
    "    all_accs[file] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('EXTRA.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    writer.writerow(cols)\n",
    "    for file in tqdm(all_accs):\n",
    "        for line in all_accs[file]:\n",
    "            writer.writerow(list(line.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
